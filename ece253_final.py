# -*- coding: utf-8 -*-
"""ECE253_repo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17boeB2Fi7opHBrBFZGE42HAnY6sV9slp
"""

# -*- coding: utf-8 -*-
"""
ECE253 — Complete Three-Distortion Analysis

Project: Enhancing Object Detection Performance on Degraded Images
Team members: Xin Zhao, Cheng Qian, Pengfei Zhan

Distortions covered:
1) Extreme low-light (40 real images)
2) Motion blur (20 real images)
3) Gaussian noise (30 images synthesized from clean)

For each distortion, this script compares:
- A traditional image-processing baseline
- An ML-based approach (fine-tuning)
- Side-by-side results on the same test set
"""


# ========================================================================
# STEP 1: Setup and Installation
# ========================================================================
print("="*70)
print("ECE253: THREE-DISTORTION COMPLETE ANALYSIS")
print("="*70)
print("\nInstalling dependencies...")

# Install required packages
import subprocess
import sys

packages = [
    'torch', 'torchvision', 'pillow', 'matplotlib',
    'opencv-python', 'scikit-image', 'xmltodict',
    'tqdm', 'albumentations', 'numpy'
]

for package in packages:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

print("Dependencies installed")

# ========================================================================
# IMPORTS
# ========================================================================
import torch
import torchvision
from torchvision.models.detection import ssd300_vgg16, SSD300_VGG16_Weights
from torchvision import transforms as T
from torchvision.ops import box_iou
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import cv2
import os
import shutil
import json
import random
import xmltodict
from pathlib import Path
from tqdm.auto import tqdm
from collections import defaultdict
import albumentations as A
from albumentations.pytorch import ToTensorV2
from skimage.restoration import denoise_nl_means, estimate_sigma
from skimage import img_as_float, img_as_ubyte
from scipy.signal import wiener
import warnings
warnings.filterwarnings('ignore')

# Set seeds for reproducibility
torch.manual_seed(253)
np.random.seed(253)
random.seed(253)

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n“Using device: {device}")
if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")

# ========================================================================
# STEP 2: Dataset Upload and Organization
# ========================================================================
print("\n" + "="*70)
print("STEP 2: Dataset Organization")
print("="*70)

# For Google Colab - upload dataset
from google.colab import files
print("\nUpload your PASCAL VOC ZIP file...")
uploaded = files.upload()

zip_filename = list(uploaded.keys())[0]
print(f"\n“Uploaded: {zip_filename}")

# Extract and organize
print("\nExtracting and organizing...")
os.system(f'unzip -q "{zip_filename}"')
os.makedirs('data.voc', exist_ok=True)
os.system('mv ./train/ ./data.voc/')
os.system('mv ./valid/ ./data.voc/')

voc_dir = 'data.voc'
print(f"✓ VOC directory: {voc_dir}")

# ========================================================================
# STEP 3: Separate Distortion Types
# ========================================================================
print("\n" + "="*70)
print("STEP 3: Separating Distortion Types")
print("="*70)

def separate_distortions(dataset_path):
    """Separate images by distortion type based on filename"""

    all_images = list(Path(dataset_path).glob('*.jpg'))

    dark_images = [img for img in all_images if 'dark' in img.name.lower()]
    blur_images = [img for img in all_images if 'blur' in img.name.lower()]
    clean_images = [img for img in all_images if 'clean' in img.name.lower()]

    return {
        'dark': dark_images,
        'blur': blur_images,
        'clean': clean_images
    }

# Separate clean and distorted images
clean_images = separate_distortions(f'{voc_dir}/train')['clean']
distorted = separate_distortions(f'{voc_dir}/valid')

dark_images = distorted['dark']
blur_images = distorted['blur']

print(f"\n✓ Dataset Statistics:")
print(f"  Clean images: {len(clean_images)}")
print(f"  Dark images:  {len(dark_images)}")
print(f"  Blur images:  {len(blur_images)}")

# ========================================================================
# STEP 4: Synthesize Gaussian Noise Distortion
# ========================================================================
print("\n" + "="*70)
print("STEP 4: Creating Gaussian Noise Dataset")
print("="*70)

def add_gaussian_noise(image, noise_level=25):
    """Add Gaussian noise to image (noise_level in range 0-255)"""
    img_array = np.array(image)
    noise = np.random.normal(0, noise_level, img_array.shape)
    noisy_img = np.clip(img_array + noise, 0, 255).astype(np.uint8)
    return Image.fromarray(noisy_img)

# Create noisy dataset directory
noisy_dir = Path(f'{voc_dir}/noisy')
noisy_dir.mkdir(exist_ok=True)

# Select 30 clean images for noise addition
selected_clean = random.sample(clean_images, min(30, len(clean_images)))

print("\nGenerating noisy images...")
for img_path in tqdm(selected_clean, desc="Adding Gaussian noise"):
    # Load image
    img = Image.open(img_path).convert('RGB')

    # Add noise
    noisy_img = add_gaussian_noise(img, noise_level=30)

    # Save with new name
    new_name = img_path.name.replace('clean', 'noisy')
    noisy_path = noisy_dir / new_name
    noisy_img.save(noisy_path)

    # Copy annotation
    xml_src = img_path.with_suffix('.xml')
    if xml_src.exists():
        xml_dst = noisy_path.with_suffix('.xml')
        shutil.copy(xml_src, xml_dst)

noisy_images = list(noisy_dir.glob('*.jpg'))
print(f"\n✓ Created {len(noisy_images)} noisy images")

# ========================================================================
# STEP 5: COCO Classes and Dataset Class
# ========================================================================
print("\n" + "="*70)
print("STEP 5: Setting up Dataset Classes")
print("="*70)

COCO_CLASSES = {
    'banana': 52,
    'apple': 53,
    'orange': 55
}

COCO_TO_NAME = {v: k for k, v in COCO_CLASSES.items()}

class FruitVOCDataset(Dataset):
    """Dataset for specific image list with PASCAL VOC annotations"""

    def __init__(self, image_paths, transform=None):
        self.images = sorted(image_paths)
        self.transform = transform
        self.annotations = []

        for img_path in self.images:
            xml_path = img_path.with_suffix('.xml')
            if xml_path.exists():
                self.annotations.append(self._parse_xml(xml_path))
            else:
                self.annotations.append({'boxes': [], 'labels': []})

    def _parse_xml(self, xml_path):
        with open(xml_path, 'r') as f:
            data = xmltodict.parse(f.read())

        boxes = []
        labels = []

        if 'object' in data['annotation']:
            objects = data['annotation']['object']
            if not isinstance(objects, list):
                objects = [objects]

            for obj in objects:
                name = obj['name'].lower()
                if name in COCO_CLASSES:
                    bbox = obj['bndbox']
                    xmin = float(bbox['xmin'])
                    ymin = float(bbox['ymin'])
                    xmax = float(bbox['xmax'])
                    ymax = float(bbox['ymax'])

                    boxes.append([xmin, ymin, xmax, ymax])
                    labels.append(COCO_CLASSES[name])

        return {
            'boxes': torch.tensor(boxes, dtype=torch.float32),
            'labels': torch.tensor(labels, dtype=torch.int64)
        }

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        img = Image.open(img_path).convert('RGB')
        annotation = self.annotations[idx]

        if self.transform:
            img = self.transform(img)

        return img, annotation, str(img_path)

# Create datasets for each distortion type
clean_dataset = FruitVOCDataset(clean_images)
dark_dataset = FruitVOCDataset(dark_images)
blur_dataset = FruitVOCDataset(blur_images)
noisy_dataset = FruitVOCDataset(noisy_images)

print(f"\nDatasets created:")
print(f"  Clean: {len(clean_dataset)} images")
print(f"  Dark:  {len(dark_dataset)} images")
print(f"  Blur:  {len(blur_dataset)} images")
print(f"  Noisy: {len(noisy_dataset)} images")

# ========================================================================
# STEP 6: Traditional Processing Algorithms
# ========================================================================
print("\n" + "="*70)
print("STEP 6: Defining Traditional Processing Algorithms")
print("="*70)

def process_lowlight(img, gamma=0.4, clahe_clip=4.0):
    """Traditional processing for low-light images"""
    img_np = np.array(img)

    # Gamma correction
    img_float = img_np.astype(np.float32) / 255.0
    img_float = np.power(img_float, gamma)
    img_np = (img_float * 255).astype(np.uint8)

    # CLAHE
    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clahe_clip, tileGridSize=(8, 8))
    l = clahe.apply(l)
    lab = cv2.merge([l, a, b])
    img_np = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

    return Image.fromarray(img_np)

def process_motion_blur(img, kernel_size=15, snr=0.01):
    """Traditional processing for motion blur using sharpening"""
    img_np = np.array(img)

    # Convert to grayscale for processing
    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)

    # Apply Wiener filter (simplified approach)
    # In practice, you'd estimate the PSF, but we'll use unsharp masking

    # Unsharp masking for sharpening
    gaussian = cv2.GaussianBlur(img_np, (0, 0), 2.0)
    sharpened = cv2.addWeighted(img_np, 1.5, gaussian, -0.5, 0)

    # Additional edge enhancement
    kernel = np.array([[-1,-1,-1],
                       [-1, 9,-1],
                       [-1,-1,-1]])
    enhanced = cv2.filter2D(sharpened, -1, kernel)

    # Blend with original
    result = cv2.addWeighted(sharpened, 0.7, enhanced, 0.3, 0)

    return Image.fromarray(np.clip(result, 0, 255).astype(np.uint8))

def process_gaussian_noise(img, h=10, template_window_size=7, search_window_size=21):
    """Traditional processing for Gaussian noise using Non-Local Means"""
    img_np = np.array(img)

    # Non-local means denoising
    denoised = cv2.fastNlMeansDenoisingColored(
        img_np, None, h, h,
        template_window_size, search_window_size
    )

    # Light bilateral filter for additional smoothing
    result = cv2.bilateralFilter(denoised, 5, 30, 30)

    return Image.fromarray(result)

print("Traditional algorithms defined:")
print("  1. Low-light: Gamma correction + CLAHE")
print("  2. Motion blur: Unsharp masking + Edge enhancement")
print("  3. Gaussian noise: Non-local means + Bilateral filter")

# ========================================================================
# STEP 7: Evaluation Functions
# ========================================================================
print("\n" + "="*70)
print("STEP 7: Setting up Evaluation")
print("="*70)

def calculate_metrics(predictions, ground_truths, iou_threshold=0.5, conf_threshold=0.3):
    """Calculate mAP, precision, and recall"""
    class_names = ['banana', 'apple', 'orange']
    class_ids = [COCO_CLASSES[name] for name in class_names]

    # Store per-class predictions and ground truths
    class_predictions = {cid: [] for cid in class_ids}
    class_ground_truths = {cid: [] for cid in class_ids}

    for pred, gt in zip(predictions, ground_truths):
        pred_boxes = pred['boxes']
        pred_labels = pred['labels']
        pred_scores = pred['scores']

        gt_boxes = gt['boxes']
        gt_labels = gt['labels']

        # Filter by confidence
        mask = pred_scores >= conf_threshold
        pred_boxes = pred_boxes[mask]
        pred_labels = pred_labels[mask]
        pred_scores = pred_scores[mask]

        for cid in class_ids:
            pred_mask = pred_labels == cid
            class_pred_boxes = pred_boxes[pred_mask]
            class_pred_scores = pred_scores[pred_mask]

            gt_mask = gt_labels == cid
            class_gt_boxes = gt_boxes[gt_mask]

            class_predictions[cid].append({
                'boxes': class_pred_boxes,
                'scores': class_pred_scores
            })
            class_ground_truths[cid].append(class_gt_boxes)

    # Calculate metrics per class
    results = {}

    for class_name, cid in zip(class_names, class_ids):
        tp = fp = fn = 0

        for pred, gt in zip(class_predictions[cid], class_ground_truths[cid]):
            pred_boxes = pred['boxes']
            gt_boxes = gt
            matched_gt = set()

            for pred_box in pred_boxes:
                if len(gt_boxes) == 0:
                    fp += 1
                    continue

                ious = box_iou(pred_box.unsqueeze(0), gt_boxes)[0]
                max_iou, max_idx = ious.max(0)

                if max_iou >= iou_threshold and max_idx.item() not in matched_gt:
                    tp += 1
                    matched_gt.add(max_idx.item())
                else:
                    fp += 1

            fn += len(gt_boxes) - len(matched_gt)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        ap = precision * recall if recall > 0 else 0

        results[class_name] = {
            'precision': precision,
            'recall': recall,
            'ap': ap,
            'tp': tp,
            'fp': fp,
            'fn': fn
        }

    # Calculate mean metrics
    results['mean'] = {
        'mAP': np.mean([r['ap'] for r in results.values() if isinstance(r, dict)]),
        'mean_precision': np.mean([r['precision'] for r in results.values() if isinstance(r, dict)]),
        'mean_recall': np.mean([r['recall'] for r in results.values() if isinstance(r, dict)])
    }

    return results

def evaluate_model(model, dataset, desc="Evaluating"):
    """Evaluate model on dataset"""
    model.eval()
    predictions = []
    ground_truths = []

    with torch.no_grad():
        for i in tqdm(range(len(dataset)), desc=desc):
            img, annotation, _ = dataset[i]

            if isinstance(img, Image.Image):
                img = T.ToTensor()(img)

            img = img.unsqueeze(0).to(device)
            pred = model(img)[0]

            pred = {
                'boxes': pred['boxes'].cpu(),
                'labels': pred['labels'].cpu(),
                'scores': pred['scores'].cpu()
            }

            predictions.append(pred)
            ground_truths.append(annotation)

    metrics = calculate_metrics(predictions, ground_truths)
    return metrics, predictions

print("✓ Evaluation functions ready")

# ========================================================================
# STEP 8: Load Pretrained Model
# ========================================================================
print("\n" + "="*70)
print("STEP 8: Loading Pretrained SSD Model")
print("="*70)

base_model = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)
base_model = base_model.to(device)
base_model.eval()

print("✓ Model loaded successfully")

# ========================================================================
# STEP 9: Baseline Performance
# ========================================================================
print("\n" + "="*70)
print("STEP 9: Baseline Performance on Clean Data")
print("="*70)

clean_metrics, _ = evaluate_model(base_model, clean_dataset, "Clean baseline")

print(f"\nCLEAN BASELINE:")
print(f"  mAP:       {clean_metrics['mean']['mAP']:.3f}")
print(f"  Precision: {clean_metrics['mean']['mean_precision']:.3f}")
print(f"  Recall:    {clean_metrics['mean']['mean_recall']:.3f}")

# ========================================================================
# STEP 10: Per-Distortion Analysis
# ========================================================================
print("\n" + "="*70)
print("STEP 10: PER-DISTORTION ANALYSIS")
print("="*70)

results_summary = {
    'baseline': clean_metrics['mean'],
    'distortions': {}
}

# ========================================================================
# DISTORTION 1: LOW-LIGHT
# ========================================================================
print("\n" + "-"*70)
print("DISTORTION 1: EXTREME LOW-LIGHT")
print("-"*70)

# Split dark dataset for training/testing
dark_indices = list(range(len(dark_dataset)))
random.shuffle(dark_indices)
dark_train_size = int(0.8 * len(dark_dataset))
dark_train_idx = dark_indices[:dark_train_size]
dark_test_idx = dark_indices[dark_train_size:]

dark_train = Subset(dark_dataset, dark_train_idx)
dark_test = Subset(dark_dataset, dark_test_idx)

print(f"\nDataset split: {len(dark_train)} train, {len(dark_test)} test")

# Test original performance on dark images
print("\n1. Testing original model on dark images...")
dark_original, _ = evaluate_model(base_model, dark_test, "Dark original")
print(f"   mAP: {dark_original['mean']['mAP']:.3f}")

# Test with traditional processing
print("\n2. Testing with traditional processing (Gamma + CLAHE)...")
class ProcessedDataset(Dataset):
    def __init__(self, base_dataset, process_fn):
        self.base_dataset = base_dataset
        self.process_fn = process_fn

    def __len__(self):
        return len(self.base_dataset)

    def __getitem__(self, idx):
        img, annotation, path = self.base_dataset[idx]
        img = self.process_fn(img)
        return img, annotation, path

dark_processed = ProcessedDataset(dark_test, process_lowlight)
dark_trad_metrics, _ = evaluate_model(base_model, dark_processed, "Dark traditional")
print(f"   mAP: {dark_trad_metrics['mean']['mAP']:.3f}")

# Fine-tune for low-light
print("\n3. Fine-tuning model for low-light...")
dark_finetuned = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)
dark_finetuned = dark_finetuned.to(device)
dark_finetuned.train()

def collate_fn(batch):
    """Custom PyTorch DataLoader collate function for variable-length target annotations.
    """
    images = []
    targets = []
    for item in batch:
        img, annotation, _ = item if len(item) == 3 else (*item, None)
        if isinstance(img, Image.Image):
            img = T.ToTensor()(img)
        images.append(img)
        targets.append({
            'boxes': annotation['boxes'],
            'labels': annotation['labels']
        })
    return images, targets

dark_loader = DataLoader(dark_train, batch_size=4, shuffle=True, collate_fn=collate_fn)
optimizer = optim.SGD(dark_finetuned.parameters(), lr=0.0005, momentum=0.9)

for epoch in range(2):
    for images, targets in tqdm(dark_loader, desc=f"Epoch {epoch+1}/2"):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        try:
            loss_dict = dark_finetuned(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
        except:
            continue

dark_finetuned.eval()
dark_ml_metrics, _ = evaluate_model(dark_finetuned, dark_test, "Dark fine-tuned")
print(f"   mAP: {dark_ml_metrics['mean']['mAP']:.3f}")

# Store results
results_summary['distortions']['low_light'] = {
    'original': dark_original['mean']['mAP'],
    'traditional': dark_trad_metrics['mean']['mAP'],
    'fine_tuned': dark_ml_metrics['mean']['mAP'],
    'best': 'fine_tuned' if dark_ml_metrics['mean']['mAP'] > dark_trad_metrics['mean']['mAP'] else 'traditional'
}

# ========================================================================
# DISTORTION 2: MOTION BLUR
# ========================================================================
print("\n" + "-"*70)
print("DISTORTION 2: MOTION BLUR")
print("-"*70)

# Split blur dataset
blur_indices = list(range(len(blur_dataset)))
random.shuffle(blur_indices)
blur_train_size = int(0.8 * len(blur_dataset))
blur_train_idx = blur_indices[:blur_train_size]
blur_test_idx = blur_indices[blur_train_size:]

blur_train = Subset(blur_dataset, blur_train_idx)
blur_test = Subset(blur_dataset, blur_test_idx)

print(f"\nDataset split: {len(blur_train)} train, {len(blur_test)} test")

# Test original performance
print("\n1. Testing original model on blurred images...")
blur_original, _ = evaluate_model(base_model, blur_test, "Blur original")
print(f"   mAP: {blur_original['mean']['mAP']:.3f}")

# Test with traditional processing
print("\n2. Testing with traditional processing (Sharpening)...")
blur_processed = ProcessedDataset(blur_test, process_motion_blur)
blur_trad_metrics, _ = evaluate_model(base_model, blur_processed, "Blur traditional")
print(f"   mAP: {blur_trad_metrics['mean']['mAP']:.3f}")

# Fine-tune for motion blur
print("\n3. Fine-tuning model for motion blur...")
blur_finetuned = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)
blur_finetuned = blur_finetuned.to(device)
blur_finetuned.train()

blur_loader = DataLoader(blur_train, batch_size=4, shuffle=True, collate_fn=collate_fn)
optimizer = optim.SGD(blur_finetuned.parameters(), lr=0.0005, momentum=0.9)

for epoch in range(2):
    for images, targets in tqdm(blur_loader, desc=f"Epoch {epoch+1}/2"):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        try:
            loss_dict = blur_finetuned(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
        except:
            continue

blur_finetuned.eval()
blur_ml_metrics, _ = evaluate_model(blur_finetuned, blur_test, "Blur fine-tuned")
print(f"   mAP: {blur_ml_metrics['mean']['mAP']:.3f}")

# Store results
results_summary['distortions']['motion_blur'] = {
    'original': blur_original['mean']['mAP'],
    'traditional': blur_trad_metrics['mean']['mAP'],
    'fine_tuned': blur_ml_metrics['mean']['mAP'],
    'best': 'fine_tuned' if blur_ml_metrics['mean']['mAP'] > blur_trad_metrics['mean']['mAP'] else 'traditional'
}

# ========================================================================
# DISTORTION 3: GAUSSIAN NOISE
# ========================================================================
print("\n" + "-"*70)
print("DISTORTION 3: GAUSSIAN NOISE")
print("-"*70)

# Split noisy dataset
noisy_indices = list(range(len(noisy_dataset)))
random.shuffle(noisy_indices)
noisy_train_size = int(0.8 * len(noisy_dataset))
noisy_train_idx = noisy_indices[:noisy_train_size]
noisy_test_idx = noisy_indices[noisy_train_size:]

noisy_train = Subset(noisy_dataset, noisy_train_idx)
noisy_test = Subset(noisy_dataset, noisy_test_idx)

print(f"\nDataset split: {len(noisy_train)} train, {len(noisy_test)} test")

# Test original performance
print("\n1. Testing original model on noisy images...")
noisy_original, _ = evaluate_model(base_model, noisy_test, "Noisy original")
print(f"   mAP: {noisy_original['mean']['mAP']:.3f}")

# Test with traditional processing
print("\n2. Testing with traditional processing (NLM denoising)...")
noisy_processed = ProcessedDataset(noisy_test, process_gaussian_noise)
noisy_trad_metrics, _ = evaluate_model(base_model, noisy_processed, "Noisy traditional")
print(f"   mAP: {noisy_trad_metrics['mean']['mAP']:.3f}")

# Fine-tune for Gaussian noise
print("\n3. Fine-tuning model for Gaussian noise...")
noisy_finetuned = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)
noisy_finetuned = noisy_finetuned.to(device)
noisy_finetuned.train()

noisy_loader = DataLoader(noisy_train, batch_size=4, shuffle=True, collate_fn=collate_fn)
optimizer = optim.SGD(noisy_finetuned.parameters(), lr=0.0005, momentum=0.9)

for epoch in range(2):
    for images, targets in tqdm(noisy_loader, desc=f"Epoch {epoch+1}/2"):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        try:
            loss_dict = noisy_finetuned(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
        except:
            continue

noisy_finetuned.eval()
noisy_ml_metrics, _ = evaluate_model(noisy_finetuned, noisy_test, "Noisy fine-tuned")
print(f"   mAP: {noisy_ml_metrics['mean']['mAP']:.3f}")

# Store results
results_summary['distortions']['gaussian_noise'] = {
    'original': noisy_original['mean']['mAP'],
    'traditional': noisy_trad_metrics['mean']['mAP'],
    'fine_tuned': noisy_ml_metrics['mean']['mAP'],
    'best': 'fine_tuned' if noisy_ml_metrics['mean']['mAP'] > noisy_trad_metrics['mean']['mAP'] else 'traditional'
}

# ========================================================================
# STEP 10.5: ABLATION STUDY - Parameter Sensitivity Analysis
# ========================================================================
print("\n" + "="*70)
print("STEP 10.5: ABLATION STUDY - Parameter Sensitivity Analysis")
print("="*70)
print("\nAnalyzing how different parameter settings affect traditional processing performance...")

# Store ablation results
ablation_results = {
    'low_light': {},
    'motion_blur': {},
    'gaussian_noise': {}
}

# ========================================================================
# ABLATION 1: Low-Light Processing Parameters
# ========================================================================
print("\n" + "-"*70)
print("ABLATION 1: Low-Light Enhancement Parameters")
print("-"*70)
print("\nTesting different combinations of gamma and CLAHE clip limit...")

# Define parameter combinations to test
lowlight_params = [
    {'name': 'Weak', 'gamma': 0.5, 'clahe_clip': 2.0},
    {'name': 'Default', 'gamma': 0.4, 'clahe_clip': 4.0},
    {'name': 'Strong', 'gamma': 0.3, 'clahe_clip': 6.0},
    {'name': 'Very Strong', 'gamma': 0.2, 'clahe_clip': 8.0},
]

for params in lowlight_params:
    print(f"\n  Testing {params['name']}: gamma={params['gamma']}, clahe_clip={params['clahe_clip']}")

    # Create processing function with these parameters
    def process_with_params(img):
        return process_lowlight(img, gamma=params['gamma'], clahe_clip=params['clahe_clip'])

    # Test on dark dataset
    dark_param_dataset = ProcessedDataset(dark_test, process_with_params)
    param_metrics, _ = evaluate_model(base_model, dark_param_dataset,
                                     f"Dark {params['name']}")

    ablation_results['low_light'][params['name']] = {
        'gamma': params['gamma'],
        'clahe_clip': params['clahe_clip'],
        'mAP': param_metrics['mean']['mAP'],
        'precision': param_metrics['mean']['mean_precision'],
        'recall': param_metrics['mean']['mean_recall']
    }

    print(f"    mAP: {param_metrics['mean']['mAP']:.3f}")

print("\nLow-Light Ablation Summary:")
for name, result in ablation_results['low_light'].items():
    print(f"  {name:12s}: mAP={result['mAP']:.3f}, "
          f"Precision={result['precision']:.3f}, Recall={result['recall']:.3f}")

# Find best parameters
best_lowlight = max(ablation_results['low_light'].items(),
                   key=lambda x: x[1]['mAP'])
print(f"\nBest Low-Light Parameters: {best_lowlight[0]} "
      f"(gamma={best_lowlight[1]['gamma']}, clahe_clip={best_lowlight[1]['clahe_clip']})")

# ========================================================================
# ABLATION 2: Motion Blur Processing Parameters
# ========================================================================
print("\n" + "-"*70)
print("ABLATION 2: Motion Blur Sharpening Parameters")
print("-"*70)
print("\nTesting different sharpening intensities...")

# Define parameter combinations for blur
blur_params = [
    {'name': 'Weak', 'unsharp_weight': 1.2, 'edge_weight': 0.1},
    {'name': 'Default', 'unsharp_weight': 1.5, 'edge_weight': 0.3},
    {'name': 'Strong', 'unsharp_weight': 1.8, 'edge_weight': 0.5},
    {'name': 'Very Strong', 'unsharp_weight': 2.0, 'edge_weight': 0.7},
]

# Create parameterized blur processing function
def process_motion_blur_params(img, unsharp_weight=1.5, edge_weight=0.3):
    """Parameterized motion blur processing"""
    img_np = np.array(img)

    # Unsharp masking for sharpening
    gaussian = cv2.GaussianBlur(img_np, (0, 0), 2.0)
    sharpened = cv2.addWeighted(img_np, unsharp_weight, gaussian, -(unsharp_weight-1), 0)

    # Edge enhancement
    kernel = np.array([[-1,-1,-1],
                       [-1, 9,-1],
                       [-1,-1,-1]])
    enhanced = cv2.filter2D(sharpened, -1, kernel)

    # Blend
    result = cv2.addWeighted(sharpened, 1-edge_weight, enhanced, edge_weight, 0)

    return Image.fromarray(np.clip(result, 0, 255).astype(np.uint8))

for params in blur_params:
    print(f"\n  Testing {params['name']}: unsharp_weight={params['unsharp_weight']}, "
          f"edge_weight={params['edge_weight']}")

    # Create processing function with these parameters
    def process_with_params(img):
        return process_motion_blur_params(img,
                                         unsharp_weight=params['unsharp_weight'],
                                         edge_weight=params['edge_weight'])

    # Test on blur dataset
    blur_param_dataset = ProcessedDataset(blur_test, process_with_params)
    param_metrics, _ = evaluate_model(base_model, blur_param_dataset,
                                     f"Blur {params['name']}")

    ablation_results['motion_blur'][params['name']] = {
        'unsharp_weight': params['unsharp_weight'],
        'edge_weight': params['edge_weight'],
        'mAP': param_metrics['mean']['mAP'],
        'precision': param_metrics['mean']['mean_precision'],
        'recall': param_metrics['mean']['mean_recall']
    }

    print(f"    mAP: {param_metrics['mean']['mAP']:.3f}")

print("\nMotion Blur Ablation Summary:")
for name, result in ablation_results['motion_blur'].items():
    print(f"  {name:12s}: mAP={result['mAP']:.3f}, "
          f"Precision={result['precision']:.3f}, Recall={result['recall']:.3f}")

# Find best parameters
best_blur = max(ablation_results['motion_blur'].items(),
               key=lambda x: x[1]['mAP'])
print(f"\nBest Motion Blur Parameters: {best_blur[0]} "
      f"(unsharp_weight={best_blur[1]['unsharp_weight']}, "
      f"edge_weight={best_blur[1]['edge_weight']})")

# ========================================================================
# ABLATION 3: Gaussian Noise Processing Parameters
# ========================================================================
print("\n" + "-"*70)
print("ABLATION 3: Gaussian Noise Denoising Parameters")
print("-"*70)
print("\nTesting different denoising strengths...")

# Define parameter combinations for noise
noise_params = [
    {'name': 'Weak', 'h': 5, 'template_size': 5, 'search_size': 15},
    {'name': 'Default', 'h': 10, 'template_size': 7, 'search_size': 21},
    {'name': 'Strong', 'h': 15, 'template_size': 7, 'search_size': 25},
    {'name': 'Very Strong', 'h': 20, 'template_size': 9, 'search_size': 31},
]

# Create parameterized noise processing function
def process_gaussian_noise_params(img, h=10, template_size=7, search_size=21):
    """Parameterized Gaussian noise processing"""
    img_np = np.array(img)

    # Non-local means denoising
    denoised = cv2.fastNlMeansDenoisingColored(
        img_np, None, h, h,
        template_size, search_size
    )

    # Light bilateral filter
    result = cv2.bilateralFilter(denoised, 5, 30, 30)

    return Image.fromarray(result)

for params in noise_params:
    print(f"\n  Testing {params['name']}: h={params['h']}, "
          f"template_size={params['template_size']}, search_size={params['search_size']}")

    # Create processing function with these parameters
    def process_with_params(img):
        return process_gaussian_noise_params(img,
                                            h=params['h'],
                                            template_size=params['template_size'],
                                            search_size=params['search_size'])

    # Test on noisy dataset
    noise_param_dataset = ProcessedDataset(noisy_test, process_with_params)
    param_metrics, _ = evaluate_model(base_model, noise_param_dataset,
                                     f"Noise {params['name']}")

    ablation_results['gaussian_noise'][params['name']] = {
        'h': params['h'],
        'template_size': params['template_size'],
        'search_size': params['search_size'],
        'mAP': param_metrics['mean']['mAP'],
        'precision': param_metrics['mean']['mean_precision'],
        'recall': param_metrics['mean']['mean_recall']
    }

    print(f"    mAP: {param_metrics['mean']['mAP']:.3f}")

print("\nGaussian Noise Ablation Summary:")
for name, result in ablation_results['gaussian_noise'].items():
    print(f"  {name:12s}: mAP={result['mAP']:.3f}, "
          f"Precision={result['precision']:.3f}, Recall={result['recall']:.3f}")

# Find best parameters
best_noise = max(ablation_results['gaussian_noise'].items(),
                key=lambda x: x[1]['mAP'])
print(f"\nBest Gaussian Noise Parameters: {best_noise[0]} "
      f"(h={best_noise[1]['h']}, template_size={best_noise[1]['template_size']}, "
      f"search_size={best_noise[1]['search_size']})")

# ========================================================================
# Create Ablation Study Visualization
# ========================================================================
print("\n" + "-"*70)
print("Creating Ablation Study Visualization")
print("-"*70)

fig, axes = plt.subplots(1, 3, figsize=(18, 6))
fig.suptitle('Ablation Study: Parameter Sensitivity Analysis',
             fontsize=16, fontweight='bold')

# Plot 1: Low-Light parameters
ax1 = axes[0]
names = list(ablation_results['low_light'].keys())
maps = [ablation_results['low_light'][n]['mAP'] for n in names]
bars = ax1.bar(names, maps, color='darkblue', alpha=0.7, edgecolor='black')

# Highlight best
best_idx = maps.index(max(maps))
bars[best_idx].set_color('gold')
bars[best_idx].set_edgecolor('darkgreen')
bars[best_idx].set_linewidth(3)

for bar, val in zip(bars, maps):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')

ax1.set_title('Low-Light Enhancement', fontsize=13, fontweight='bold')
ax1.set_ylabel('mAP', fontsize=11)
ax1.set_ylim([0, max(maps) * 1.3])
ax1.tick_params(axis='x', rotation=15)
ax1.grid(axis='y', alpha=0.3)

# Plot 2: Motion Blur parameters
ax2 = axes[1]
names = list(ablation_results['motion_blur'].keys())
maps = [ablation_results['motion_blur'][n]['mAP'] for n in names]
bars = ax2.bar(names, maps, color='darkorange', alpha=0.7, edgecolor='black')

# Highlight best
best_idx = maps.index(max(maps))
bars[best_idx].set_color('gold')
bars[best_idx].set_edgecolor('darkgreen')
bars[best_idx].set_linewidth(3)

for bar, val in zip(bars, maps):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')

ax2.set_title('Motion Blur Sharpening', fontsize=13, fontweight='bold')
ax2.set_ylabel('mAP', fontsize=11)
ax2.set_ylim([0, max(maps) * 1.3])
ax2.tick_params(axis='x', rotation=15)
ax2.grid(axis='y', alpha=0.3)

# Plot 3: Gaussian Noise parameters
ax3 = axes[2]
names = list(ablation_results['gaussian_noise'].keys())
maps = [ablation_results['gaussian_noise'][n]['mAP'] for n in names]
bars = ax3.bar(names, maps, color='purple', alpha=0.7, edgecolor='black')

# Highlight best
best_idx = maps.index(max(maps))
bars[best_idx].set_color('gold')
bars[best_idx].set_edgecolor('darkgreen')
bars[best_idx].set_linewidth(3)

for bar, val in zip(bars, maps):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')

ax3.set_title('Gaussian Noise Denoising', fontsize=13, fontweight='bold')
ax3.set_ylabel('mAP', fontsize=11)
ax3.set_ylim([0, max(maps) * 1.3])
ax3.tick_params(axis='x', rotation=15)
ax3.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('ablation_study.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nSaved ablation_study.png")

# ========================================================================
# ABLATION STUDY CONCLUSIONS
# ========================================================================
print("\n" + "="*70)
print("ABLATION STUDY CONCLUSIONS")
print("="*70)

print("\nKey Findings:")
print("\n1. LOW-LIGHT ENHANCEMENT:")
print(f"   Best: {best_lowlight[0]} (mAP: {best_lowlight[1]['mAP']:.3f})")
print("   Observation: Stronger gamma correction can amplify noise in extreme low-light")

print("\n2. MOTION BLUR SHARPENING:")
print(f"   Best: {best_blur[0]} (mAP: {best_blur[1]['mAP']:.3f})")
print("   Observation: Moderate sharpening works best; too strong creates artifacts")

print("\n3. GAUSSIAN NOISE DENOISING:")
print(f"   Best: {best_noise[0]} (mAP: {best_noise[1]['mAP']:.3f})")
print("   Observation: Stronger denoising improves detection but may blur edges")

# Store ablation results in summary
results_summary['ablation_study'] = ablation_results

# ========================================================================
# STEP 11: Albumentations Testing
# ========================================================================
print("\n" + "="*70)
print("STEP 11: Testing Albumentations Library")
print("="*70)

# Create augmentation pipeline with albumentations
transform_album = A.Compose([
    A.OneOf([
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),
        A.RandomGamma(gamma_limit=(80, 120), p=0.5),
        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5)
    ], p=0.9),
    A.OneOf([
        A.Blur(blur_limit=3, p=0.5),
        A.MedianBlur(blur_limit=3, p=0.5),
        A.GaussianBlur(blur_limit=3, p=0.5)
    ], p=0.5),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
])

print("\nTesting albumentations augmentations on dark images...")

class AlbumentationsDataset(Dataset):
    def __init__(self, base_dataset, transform):
        self.base_dataset = base_dataset
        self.transform = transform

    def __len__(self):
        return len(self.base_dataset)

    def __getitem__(self, idx):
        img, annotation, path = self.base_dataset[idx]
        img_np = np.array(img)

        # Apply augmentation
        augmented = self.transform(image=img_np)
        img_aug = Image.fromarray(augmented['image'])

        return img_aug, annotation, path

# Test on subset of dark images
dark_album = AlbumentationsDataset(dark_test, transform_album)
album_metrics, _ = evaluate_model(base_model, dark_album, "Albumentations")

print(f"\nAlbumentations enhancement on dark images:")
print(f"  Original mAP:      {dark_original['mean']['mAP']:.3f}")
print(f"  Albumentations mAP: {album_metrics['mean']['mAP']:.3f}")
print(f"  Improvement:       {(album_metrics['mean']['mAP'] - dark_original['mean']['mAP'])*100:.1f}%")

# ========================================================================
# STEP 12: Visualization
# ========================================================================
print("\n" + "="*70)
print("STEP 12: Creating Visualizations")
print("="*70)

# Create comprehensive comparison chart
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, (distortion_name, distortion_data) in enumerate(results_summary['distortions'].items()):
    ax = axes[idx]

    methods = ['Original', 'Traditional', 'Fine-tuned']
    values = [
        distortion_data['original'],
        distortion_data['traditional'],
        distortion_data['fine_tuned']
    ]
    colors = ['red', 'blue', 'green']

    bars = ax.bar(methods, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)

    # Add value labels
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    ax.set_title(f'{distortion_name.replace("_", " ").title()}', fontsize=14, fontweight='bold')
    ax.set_ylabel('mAP', fontsize=12)
    ax.set_ylim([0, max(values) * 1.2])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # Highlight best method
    best_idx = values.index(max(values))
    bars[best_idx].set_edgecolor('gold')
    bars[best_idx].set_linewidth(3)

plt.suptitle('Per-Distortion Performance Comparison', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('three_distortions_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print("✓ Saved three_distortions_comparison.png")

# Create summary table
fig, ax = plt.subplots(figsize=(12, 6))
ax.axis('tight')
ax.axis('off')

# Prepare table data
table_data = [['Distortion', 'Original mAP', 'Traditional mAP', 'Fine-tuned mAP', 'Best Method']]
for name, data in results_summary['distortions'].items():
    row = [
        name.replace('_', ' ').title(),
        f"{data['original']:.3f}",
        f"{data['traditional']:.3f}",
        f"{data['fine_tuned']:.3f}",
        data['best'].replace('_', '-').title()
    ]
    table_data.append(row)

table = ax.table(cellText=table_data, loc='center', cellLoc='center')
table.auto_set_font_size(False)
table.set_fontsize(11)
table.scale(1.2, 1.8)

# Style header row
for i in range(len(table_data[0])):
    table[(0, i)].set_facecolor('#4CAF50')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Style best method column
for i in range(1, len(table_data)):
    if 'fine' in table_data[i][4].lower():
        table[(i, 4)].set_facecolor('#90EE90')
    else:
        table[(i, 4)].set_facecolor('#FFE4B5')

plt.title('Complete Results Summary', fontsize=14, fontweight='bold', pad=20)
plt.savefig('results_summary_table.png', dpi=150, bbox_inches='tight')
plt.show()

print("✓ Saved results_summary_table.png")

# ========================================================================
# STEP 13: Save Final Results
# ========================================================================
print("\n" + "="*70)
print("STEP 13: Saving Final Results")
print("="*70)

# Save JSON results
final_results = {
    'project': 'ECE253 - Enhancing Object Detection on Degraded Images',
    'team': ['Xin Zhao', 'Cheng Qian', 'Pengfei Zhan'],
    'distortions': results_summary['distortions'],
    'ablation_study': results_summary.get('ablation_study', {}),
    'conclusions': {
        'low_light': f"Best: {results_summary['distortions']['low_light']['best']}",
        'motion_blur': f"Best: {results_summary['distortions']['motion_blur']['best']}",
        'gaussian_noise': f"Best: {results_summary['distortions']['gaussian_noise']['best']}"
    },
    'ablation_best_params': {
        'low_light': {
            'params': best_lowlight[0],
            'gamma': best_lowlight[1]['gamma'],
            'clahe_clip': best_lowlight[1]['clahe_clip'],
            'mAP': best_lowlight[1]['mAP']
        },
        'motion_blur': {
            'params': best_blur[0],
            'unsharp_weight': best_blur[1]['unsharp_weight'],
            'edge_weight': best_blur[1]['edge_weight'],
            'mAP': best_blur[1]['mAP']
        },
        'gaussian_noise': {
            'params': best_noise[0],
            'h': best_noise[1]['h'],
            'template_size': best_noise[1]['template_size'],
            'search_size': best_noise[1]['search_size'],
            'mAP': best_noise[1]['mAP']
        }
    },
    'albumentations_test': {
        'dark_original': dark_original['mean']['mAP'],
        'dark_albumentations': album_metrics['mean']['mAP']
    }
}

with open('three_distortions_results.json', 'w') as f:
    json.dump(final_results, f, indent=2)

print("✓ Saved three_distortions_results.json")

# ========================================================================
# FINAL SUMMARY
# ========================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

for name, data in results_summary['distortions'].items():
    print(f"\n{name.replace('_', ' ').upper()}:")
    print(f"  Original:    {data['original']:.3f} mAP")
    print(f"  Traditional: {data['traditional']:.3f} mAP ({(data['traditional']-data['original'])*100:+.1f}%)")
    print(f"  Fine-tuned:  {data['fine_tuned']:.3f} mAP ({(data['fine_tuned']-data['original'])*100:+.1f}%)")
    print(f"  -> WINNER: {data['best'].replace('_', '-').title()}")

print("\n" + "="*70)
print(" EXPERIMENT COMPLETE!")
print("="*70)

# Download results
from google.colab import files
print("\nDownloading results...")
for filename in ['three_distortions_comparison.png', 'results_summary_table.png', 'ablation_study.png', 'three_distortions_results.json']:
    if os.path.exists(filename):
        files.download(filename)
        print(f"  ✓ {filename}")

print("\nAll results downloaded successfully!")